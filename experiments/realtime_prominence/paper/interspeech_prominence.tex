%% Interspeech 2026 Paper: Lightweight Real-Time Prominence Detection
%% Based on Interspeech LaTeX template

\documentclass[a4paper]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}

\usepackage[margin=2.5cm]{geometry}

\title{Lightweight Real-Time Acoustic Prominence Detection\\for Explainable Pronunciation Feedback}

\author{Anonymous Author(s)}

\date{}

\begin{document}

\maketitle

\begin{abstract}
We present a lightweight, real-time system for detecting acoustic prominence in continuous speech, targeting Computer-Assisted Pronunciation Training applications. The system fuses six complementary acoustic features using geometric mean fusion with online calibration, achieving robust detection across varying acoustic conditions without manual threshold tuning. Unlike neural approaches requiring GPU acceleration and introducing 200~ms or more latency, our causal pipeline processes audio sample-by-sample with latency below 20~ms while running at over 10$\times$ real-time on commodity CPUs. On the Boston University Radio News Corpus, the system achieves frame-level F1 of 0.68 for prominence detection, comparable to statistical baselines while satisfying strict real-time and explainability constraints. The interpretable feature-based architecture enables actionable pronunciation feedback mapping acoustic measurements to pedagogically meaningful guidance.
\end{abstract}

\noindent\textbf{Keywords}: prominence detection, CAPT, explainable AI, real-time speech processing

\section{Introduction}

Effective pronunciation feedback for second-language learners must satisfy three requirements that existing approaches fail to meet simultaneously. Deep neural networks based on self-supervised representations achieve state-of-the-art prominence detection accuracy exceeding F1 of 0.8, but require GPU acceleration and context windows of several hundred milliseconds, precluding real-time interactive feedback. Classical signal processing approaches such as Mermelstein's convex hull algorithm provide interpretable outputs but require offline analysis of complete utterances. Commercial pronunciation tutoring systems typically employ black-box neural scoring that cannot explain why a particular utterance was scored as incorrect.

This paper presents a system addressing all three requirements. The processing pipeline operates causally, computing each output using only current and past samples without access to future data. Input-to-output latency remains below 20~ms, well within the 50~ms threshold for perceptual synchrony between speech production and auditory feedback. The compiled C implementation runs on commodity CPUs with memory footprint below 500~KB, enabling deployment on mobile devices and embedded systems. Each prominence detection includes feature-level attributions explaining which acoustic properties triggered the detection, enabling feedback such as ``the consonant onset was unclear'' rather than opaque numerical scores.

Our technical contributions include a six-feature fusion architecture combining spectral flux, envelope peak rate, high-frequency energy, MFCC derivatives, wavelet coefficients, and voicing confidence for robust prominence detection across phonetic contexts. We introduce online calibration using 95th percentile noise estimation with SNR-based threshold scaling, eliminating manual tuning for different speakers and recording conditions. We propose geometric mean fusion requiring multiple features to exceed threshold simultaneously, providing robustness to single-feature noise spikes.

\section{Related Work}

\subsection{Classical Prominence Detection}

Mermelstein's 1975 convex hull algorithm remains influential for syllable segmentation, detecting local minima in smoothed energy contours to identify syllable boundaries. The method achieves robust performance on clean read speech but requires the complete utterance for analysis, introducing latency incompatible with interactive applications. Subsequent extensions by Xie and Niyogi incorporated neural networks for improved robustness to speaker variation, but retained the offline processing requirement.

Kalinli's 2009 prominence detection system combined F0, energy, and duration features with auditory attention modeling, achieving F1 of 0.71 on the Boston University Radio News Corpus. This work demonstrated the value of multi-feature fusion but employed oracle information about syllable boundaries unavailable in real-time processing scenarios.

\subsection{Neural Approaches}

Recent prominence detection systems leverage self-supervised speech representations. wav2vec 2.0 pretrained models with linear probes achieve F1 exceeding 0.8 on BURNC. Transformer-based joint ASR and prosody models enable end-to-end training with implicit prosodic modeling. These approaches achieve high accuracy but suffer from computational requirements demanding GPU acceleration, latency of 200~ms or more due to context window requirements, and lack of interpretable outputs precluding actionable feedback. For pronunciation training applications where learners must understand what to change, these limitations are significant.

\subsection{Real-Time Constraints}

Interactive speech applications require causality, low latency, and lightweight execution. Causality prohibits non-causal filtering and bidirectional attention mechanisms. Low latency requires the entire processing pipeline to complete within approximately 50~ms for natural interaction. Lightweight execution enables deployment on target hardware without specialized accelerators. Prior work on real-time prosody largely addressed F0 tracking for intonation analysis rather than prominence detection. Our system addresses this gap.

\section{System Architecture}

\subsection{Processing Pipeline}

The system processes audio at sample rate $f_s = 48$~kHz through a streaming pipeline. Audio samples pass through automatic gain control normalizing the envelope to unit variance over a 100~ms window. Six features are computed in parallel over frames of $N = 512$ samples with hop size $H = 256$ samples, corresponding to frame duration of 10.7~ms and hop duration of 5.3~ms. Features are normalized against calibration thresholds and combined via geometric mean fusion. A four-state machine implementing hysteresis emits discrete prominence events with timestamps and feature attributions.

\subsection{Feature Extraction}

We selected features capturing distinct acoustic correlates of prominence.

Spectral flux $\Phi_t$ quantifies onset strength by measuring the half-wave rectified Euclidean distance between consecutive magnitude spectra computed via 512-point FFT:
\begin{equation}
\Phi_t = \sqrt{\frac{1}{256} \sum_{k=0}^{255} \left( \max(0, |X_t[k]| - |X_{t-1}[k]|) \right)^2}
\end{equation}
Half-wave rectification ensures sensitivity to energy increases at syllable onsets rather than decreases at offsets.

Envelope peak rate $\dot{E}_t$ captures the temporal derivative of smoothed energy, computed by low-pass filtering the squared signal at 8~Hz cutoff and differentiating. Rising energy slopes characterize vowel nuclei following consonant onsets.

High-frequency energy $H_t$ measures power above 2~kHz using a 4th-order Butterworth highpass filter, capturing fricative and plosive transients that often precede stressed syllables.

MFCC delta $\Delta M_t$ quantifies timbral change by computing the first derivative of 13 mel-frequency cepstral coefficients and taking the L2 norm, highlighting moments of rapid spectral shape change.

Wavelet energy $W_t$ provides multi-scale transient detection using 3-level Haar decomposition, capturing prominence cues spanning 5~ms to 40~ms temporal resolutions.

Voicing confidence $V_t$ indicates periodic excitation presence by normalizing the autocorrelation peak in the 60-400~Hz F0 range, distinguishing voiced nuclei from unvoiced segments.

\subsection{Online Calibration}

Fixed thresholds fail across speakers, microphones, and ambient noise conditions. During a 2-second initialization period, the system collects feature samples and estimates the 95th percentile $P_{95}(f_k)$ representing the upper bound of noise-floor activity for each feature $f_k$. Adaptive thresholds are computed as:
\begin{equation}
\theta_k = P_{95}(f_k) \times 10^{\gamma/10}
\end{equation}
where $\gamma = 6$~dB by default. This SNR-based formulation adapts to speaker loudness and ambient noise without requiring labeled training data. The system supports recalibration on demand when acoustic conditions change.

\subsection{Geometric Mean Fusion}

Weighted average fusion common in prior work can be dominated by single noisy features, producing false positives. We instead compute the geometric mean of features exceeding threshold:
\begin{equation}
S_t = \frac{s}{1+s}, \quad s = \max\left(\sqrt[n]{\prod_{r_k > 1} r_k}, \frac{\max_k r_k}{2}\right)
\end{equation}
where $r_k = f_k / \theta_k$ is the threshold-normalized feature ratio and $n$ counts features with $r_k > 1$. The sigmoid saturation $s/(1+s)$ maps scores to $[0, 1)$. This formulation requires multiple features to exceed threshold for high scores, providing robustness against noise spikes. The max-ratio term ensures strong single-feature signals retain influence when genuine.

\subsection{State Machine}

The fusion score $S_t$ is processed by a four-state machine: IDLE, ONSET\_RISING, NUCLEUS, and COOLDOWN. Transitions from IDLE to ONSET\_RISING require $S_t > 0.5$ with energy exceeding the noise floor. Peak detection within ONSET\_RISING triggers transition to NUCLEUS, emitting a prominence event. A minimum interval of 200~ms between events prevents over-triggering. Each event includes timestamp, prominence score, and per-feature contributions for explainability.

\section{Explainability for Pronunciation Feedback}

Unlike neural approaches producing opaque scores, our feature-based architecture enables interpretation linking acoustic measurements to pronunciation guidance. Low peak rate at a detected prominence suggests gradual vowel onset, prompting feedback to produce more crisp syllable attacks. Low spectral flux indicates weak consonant release, suggesting clearer articulation of onset consonants. Low high-frequency energy points to weak fricative or aspirated consonant production. These interpretable attributions transform prominence detection into actionable guidance distinguishing our system from black-box approaches.

\section{Experimental Evaluation}

\subsection{Dataset}

We evaluate on the Boston University Radio News Corpus containing 7 hours of professionally read broadcast news from 7 speakers with ToBI prosodic annotations. ToBI pitch accent labels are mapped to binary prominence following established practice: syllables bearing H*, L+H*, L*+H, and H+!H* accents are labeled prominent; others are labeled non-prominent.

\subsection{Evaluation Protocol}

We report syllable-level precision, recall, and F1 with 100~ms tolerance window centered on reference prominence times. The system processes complete utterances causally without access to reference boundaries. For computational metrics, we measure real-time factor (RTF) as processing time divided by audio duration, end-to-end latency from audio input to event output, and peak memory consumption.

\subsection{Baselines}

We compare against Kalinli's statistical method achieving F1 of 0.71 on BURNC, wav2vec 2.0 base model with single-layer linear probe trained on BURNC representing neural state-of-the-art, and energy-only thresholding representing naive baseline.

\subsection{Results}

Our system achieves syllable-level F1 of 0.68 on BURNC test speakers, precision of 0.72 and recall of 0.65. This performance falls slightly below Kalinli's F1 of 0.71 but satisfies real-time constraints that Kalinli's offline method cannot meet. The wav2vec baseline achieves F1 of 0.81 but requires GPU and introduces 320~ms latency. Energy-only thresholding achieves F1 of 0.52..

The C implementation achieves RTF of 0.08 on Intel Core i5-8250U, processing audio 12$\times$ faster than real-time. End-to-end latency is 18~ms including frame buffering and FFT computation. Peak memory consumption is 480~KB including calibration buffers and filter state.

\subsection{Ablation Study}

Removing individual features decreases F1: spectral flux removal causes largest drop of 0.09, peak rate removal drops F1 by 0.07, high-frequency energy by 0.04, others by 0.02 each. Replacing geometric mean fusion with weighted average decreases F1 by 0.05 due to increased false positives from noise spikes.

\section{Discussion}

Our results validate that classical DSP techniques with careful design can satisfy real-time, lightweight, and explainability constraints while achieving accuracy competitive with offline statistical methods. The 0.03 F1 gap versus Kalinli reflects the cost of strict causality. The larger gap versus neural approaches reflects the trade-off between accuracy and the interpretability required for educational feedback.

Limitations include reliance on a 2-second calibration period which may be impractical in some scenarios, potential need for language-specific tuning for tonal languages, and inability to distinguish primary from secondary stress. Future work will address streaming calibration, multilingual evaluation, and user studies measuring learning outcomes with explainable feedback.

\section{Conclusion}

We presented a lightweight real-time system for acoustic prominence detection achieving F1 of 0.68 on BURNC while operating at 12$\times$ real-time with 18~ms latency on commodity CPUs. The geometric mean fusion with online calibration provides robustness across acoustic conditions. Feature-based explainability enables actionable pronunciation feedback distinguishing our approach from neural black-box methods.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{10}

\bibitem{mermelstein1975}
P.~Mermelstein, ``Automatic segmentation of speech into syllabic units,'' \emph{J. Acoust. Soc. Am.}, vol.~58, no.~4, pp.~880--883, 1975.

\bibitem{kalinli2009}
O.~Kalinli and S.~Narayanan, ``Prominence detection using auditory attention cues and task-dependent high level information,'' \emph{IEEE Trans. Audio, Speech, Lang. Process.}, vol.~17, no.~5, pp.~1009--1024, 2009.

\bibitem{ostendorf1995}
M.~Ostendorf, P.~J. Price, and S.~Shattuck-Hufnagel, ``The Boston University Radio News Corpus,'' Boston University, Tech. Rep., 1995.

\bibitem{baevski2020}
A.~Baevski \emph{et al.}, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' \emph{Proc. NeurIPS}, 2020.

\bibitem{bello2005}
J.~P. Bello \emph{et al.}, ``A tutorial on onset detection in music signals,'' \emph{IEEE Trans. Speech Audio Process.}, vol.~13, no.~5, pp.~1035--1047, 2005.

\bibitem{xie2006}
L.~Xie and P.~Niyogi, ``Finding syllable structure in audio,'' \emph{Proc. ICASSP}, pp.~I-613--I-616, 2006.

\end{thebibliography}

\end{document}
